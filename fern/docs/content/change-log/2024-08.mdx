---
title: Changelog | August, 2024
---

# Tool Choice Parameter support for OpenAI

_August 28th, 2024_

We are excited to announce that you can now natively specify how prompts handle functions using OpenAI's [Tool Choice](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tool_choice)
parameter. With the Tool Choice parameter, you can now dictate exactly when tools are used, allowing more precise and effective control of your prompt tools.

This feature is now available across all OpenAI models that support functions.

![Tool Choice Enablement](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/tool_choice_parameter.png)

## Add Metadata to Workflow Executions

_August 27th, 2024_

You can now add metadata to your Workflow Executions through the API. This is useful for tracking additional information
about your executions, such as the source of the request or any other custom data you want to associate with the
execution.

This metadata is visible in the Workflow Execution Details page in the Vellum UI.

You can view more information at the [API documentation](https://docs.vellum.ai/api-reference/workflows/execute-workflow#request.body.metadata).
![Workflow Execution Details Metadata](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/workflow-executions-metadata.png)

## View the Provider Payload on a Workflow's Prompt Node

_August 26th, 2024_

You can now view the compiled provider payload on a Workflow's Prompt Node. This is useful for debugging and understanding the
exact data that was sent to the provider during a run, especially if you got some unexpected results.

![Workflow Provider Payload](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/workflow-provider-payload.png)

## Merging Two Adjacent Prompt Blocks

_August 26th, 2024_

Merging two adjacent prompt blocks in the prompt editor is now possible! This feature is especially useful when you want to combine two prompt long prompt blocks into one.

You can find this button in the top right drop down in the prompt editor.

Only blocks of the same type can be merged. For example, you can merge two rich text blocks or two Jinja blocks, but you cannot merge a rich text block with a Jinja block.
You can easily convert between the two, however, by clicking the three dots in the top right of the block and selecting "Convert to Jinja" or "Convert to Rich Text".

![Merging Two Adjacent Blocks Dropdown](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/merge-two-blocks.png)

## Asynchronous Exports of Evaluation Reports

_August 26th, 2024_

Exports of evaluation reports are now asynchronous. You can export your evaluation report along with its results in CSV or JSON format, and an email will be sent to you once the export is done.

This change is especially useful for large evaluation reports, where the export process and download can take some time.

![Evaluation Report Export](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/evaluation-report-export.png)

## JSON Schema Editor with $ref Support

_August 26th, 2024_

Vellum let's you define JSON Schemas in a few different places throughout the app to do things like define [Structured Outputs](changelog/2024/2024-08#openai-structured-outputs-support) and [Function Calls](/help-center/prompts/prompt-engineering#function-calling). Previously this UI was just a simple form that allowed you to define basic JSON schemas. This UI has been improved to support direct edits via a raw JSON editor.

![Raw Schema Button](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/raw_schema_button.png)

From here, you can edit your JSON schema directly. This raw editor allows you to make use of all features supported by the [JSON Schema spec](https://json-schema.org/overview/what-is-jsonschema), even if they may not yet be supported by our basic form UI. For example, you can now defined references (i.e. `$ref`) like this:
as references:

![Raw Editor References](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/raw_editor_references.png)

## Support for Excel Files in Document Indexes

_August 23rd, 2024_

We now support uploading `.xls` and `.xlsx` files to Document Indexes for indexing and searching.

## Prompt Caching Support for Anthropic

_August 22nd, 2024_

Anthropic recently released some exciting API changes that allow for [Prompt Caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#how-prompt-caching-works).
This new feature allows for caching of frequently used portions of your Prompt for up to 5 minutes; which reduces the latency and cost of subsequent executions that include the same Prompt context.

This powerful feature is now natively supported within Vellum! In order to use it, simply toggle the new cache options on a given Prompt Block for the supported
Claude Sonnet 3.5 and Claude Haiku 3.0 models.

![Vellum Prompt Caching](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/prompt_sandbox_cache.png)

## Prompt Execution Pages

_August 22nd, 2024_

If you wanted to drill into a single Prompt Execution, previously you’d have to navigate to the Prompt Deployment's Executions table and try to filter for the specific Execution ID
you're looking for. Now each row has a navigable link accessible from the table:

![Prompt Execution Link](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/prompt-execution-link.png)

This will navigate you to a dedicated page representing that specific Prompt Execution. From here, you can see details about the Execution like the raw HTTP data sent to and from the provider,
any actuals recorded, the Vellum inputs and outputs to the prompt, and more!

![Prompt Execution Page](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/prompt-execution-page.png)

## Historical Versions of Entities in Evaluation Reports

_August 21st, 2024_

Earlier this month, we introduced [Evaluation Report History](https://docs.vellum.ai#evaluation-report-history), which allows you to view a history of all Evaluation runs and revisit the results of any prior state. We’ve now enhanced this feature by adding the ability to preview or navigate directly to the version of the Workflow or Prompt as it existed during that specific run.

![Evaluation Report History](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/evaluation-report-history-entity-linking.png)

## GPT-4o Finetuning

_August 19th, 2024_

OpenAI's newest GPT-4o models `gpt-4o-2024-08-06` and `gpt-4o-mini-2024-07-18` are now available as base models to add as OpenAI finetuned models.

## Workflow Execution Replay & Scrubbing

_August 18th, 2024_

You can now replay and scrub through the execution of a Workflow in Workflow Sandbox and Deployment Execution Details pages.
This feature is particularly useful for debugging and understanding the flow of your Workflow, especially if it
contains loops where a single node might be run more than once.

![Workflow Execution Replay & Scrubbing](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/workflow-execution-replay-and-scrubbing.gif)

## OpenAI Structured Outputs Support

_August 15th, 2024_

OpenAI released some API changes that allow their newest models to support [Structured Outputs](https://openai.com/index/introducing-structured-outputs-in-the-api/). This powerful new feature
enables developers to strictly define the expected JSON object schemas from the model as part of the response through a model parameter, or through a function call. This new functionality is now natively integrated within Vellum!

To use within the context of Function Calling, simply toggle on the `Strict` checkbox for any given Function Call:

![Function Call Strict](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/function-call-strict.png)

To enable Structured Outputs as part of a general OpenAI response, configure the `JSON Schema` setting as part of model parameters:

![JSON Schema Strict](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/json-schema-strict.png)

Both places come with upload/download functionality built into the form. Note that for function calling, this means we've reduced the scope of the upload/download to be _just_ the `Parameters`
JSON schema field. This allows schemas to be cross-compatible between either location since we are working with an [open specification](https://json-schema.org/understanding-json-schema).

![JSON Schema Strict](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/upload-download-schema.png)

## Native JSON Input Variable Support for Prompts

_August 14th, 2024_

Vellum Prompts have historically been able to accept strings and chat histories as dynamic inputs to their template.
If you wanted to operate on JSON, you'd have to pass it as a string and then parse it within the Prompt itself
(i.e. perform `json.loads()` within a Jinja Block).

Vellum Prompts now support native JSON as inputs! When you add an input variable to a Prompt, you can now select the new "JSON" type.

![JSON Variables Dropdown](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/json-variable-dropdown.png)

JSON input values will render as prettified JSON objects when referenced in Rich Text Blocks and can be operated on directly
without the need for `json.loads()` when referenced in Jinja Blocks.

## Workflow Deployment Executions Filtered to Just API Executions

_August 12th, 2024_

Our Workflow Deployment Executions page used to list all executions of a Workflow Deployment, no matter where they were invoked from. However, this
would often get confusing because you'd see a mix of results from both eval runs and production traffic in the same view.

Our Workflow Deployment Executions page now filters down to just those executions that were invoked via the API. Executions from evaluations are still accessible from within the Evaluations UI by hovering over a row and clicking the "View Workflow Details" button:

![View Workflow Details](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/view-workflow-details.png)

## Add Specific Releases to Evaluation Reports

_August 12th, 2024_

We've updated Evaluation Reports to give you more control over the releases you evaluate. Previously, you could only add the latest release of a Deployment to your reports. Now, you can select specific releases by their tag, allowing you to compare different versions within your Evaluation Reports.

![Add Deployment](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-08/evaluation-report-add-by-release-tag.png)

## Workflow Sandbox Latency

_August 9th, 2024_

You can now view the latency of Workflow Sandboxes and their Nodes. To enable viewing latency click the Workflow Sandbox settings gear icon in the top right and turn on the "View Latency" option.

![Workflow Sandbox Latency](https://storage.cloud.google.com/vellum-public/help-docs/changelogs/2024-08/workflow-latency-1.png)
![Workflow Sandbox Latency Settings](https://storage.cloud.google.com/vellum-public/help-docs/changelogs/2024-08/workflow-latency-2.png)

## Prompt Sandbox Cost Tracking

_August 9th, 2024_

You can now see the dollar cost of a Prompt's execution within both a Prompt Sandbox's Prompt Editor and Comparison Mode views.
These costs are calculated using model providers' publicly available pricing data in conjunction with the number of input/output tokens used.

![Prompt Sandbox With Cost Tracking](https://storage.cloud.google.com/vellum-public/help-docs/changelogs/2024-08/prompt_sandbox_with_cost_tracking.png)
![Prompt Sandbox Comparison With Cost Tracking](https://storage.cloud.google.com/vellum-public/help-docs/changelogs/2024-08/prompt_sandbox_comparison_with_cost_tracking.png)

If you're curious about a given model's pricing, you can view details in the Model's detail page.
![MLModel Detail Page with Billing Config](https://storage.cloud.google.com/vellum-public/help-docs/changelogs/2024-08/mlmodel_detail_page_with_cost_config.png)
Most popular models already have pricing information populated, with support for even more models following in the coming days.
Showing cost information in Prompt Sandboxes is just the first step! We'll expose cost details throughout more of Vellum over time.

## GPT-4o 2024-08-06

_August 6th, 2024_

OpenAI's newest GPT-4o model `gpt-4o-2024-08-06` is now available in Vellum and has been added to all workspaces!

## Deployment Descriptions

_August 2nd, 2024_

You can now update your Prompt and Workflow Deployments to include a human-readable description. This is useful for giving other members of your team a high-level summary
of what the Prompt or Workflow does without needing to parse through the configuration or control flow.

![Update Deployment Description](https://storage.cloud.google.com/vellum-public/help-docs/changelogs/2024-08/update-deployment-description.png)

Once set, the description will appear as part of the Deployment Details page within the Deployment Info section:

![Display Deployment Description](https://storage.cloud.google.com/vellum-public/help-docs/changelogs/2024-08/display-deployment-description.png)

## Evaluation Report History

_August 1st, 2024_

It used to be that you could only view the latest set of Evaluation results for a given Prompt or Workflow. But now,
you can view a history of all Evaluation runs and go back to view the results of any prior state.

![Evaluation Report History](https://storage.cloud.google.com/vellum-public/help-docs/changelogs/2024-08/evaluation-report-history.png)

This is particularly helpful if you want to do things like compare the results of two different Evaluation runs,
download the results of a past Evaluation run, or simply view the Test Cases that existed at that time.
