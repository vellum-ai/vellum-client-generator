---
title: Changelog | February, 2024
---

## Fireworks Function Calling Model
*February 5nd, 2024*

OpenAI's GPT models have traditionally led the way in supporting structured data generation through function calling. But late last year Fireworks AI splashed in with their own [function calling model](https://blog.fireworks.ai/fireworks-raises-the-quality-bar-with-function-calling-model-and-api-release-e7f49d1e98e9)! This model is now available in Vellum for those interested in an open source alternative to GPT.
![Fireworks Function Call Model](https://storage.googleapis.com/vellum-public/help-docs/fireworks-function-call.png)

---
## Cloning Workflow Nodes
*February 2nd, 2024*

When you hover over any node in your Workflow editor, you will see a new `Duplicate Node` icon. Clicking on this will create a new copy of a node! Never again will you need to start a node from scratch when you want to just tweak a field or two.

![Clone Nodes](https://storage.googleapis.com/vellum-public/help-docs/clone-nodes.png)

---

## Prompt Node Retries
*February 1st, 2024*

You can now detect when a Prompt Node within a Workflow errors by using a Conditional Node. Using this, you can now build out retry logic around Prompt Nodes within your Workflow! This is useful if you want to catch retryable errors (like rate limit errors from an LLM provider) and try making the call to the LLM again.

See a demo of it in action here:

<iframe src="https://www.loom.com/embed/770a1ac0bbaa4f9d9192d336eb0a191c"
  width="100%"
  height="450px"
></iframe>
