---
title: Changelog | October, 2024
---

## Prompt Caching Support for OpenAI

OpenAI recently released some new support for [Prompt Caching](https://openai.com/index/api-prompt-caching/) with certain models.
With this update, you'll now see the number of Prompt Cache Tokens used by a Prompt Deployment's executions if it's backed by an OpenAI model.
This new monitoring data can be used to help analyze your cache hit rate with OpenAI and optimize your LLM spend.


