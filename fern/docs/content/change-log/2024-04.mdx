---
title: Changelog | April, 2024
---

## Support for Evaluating External Functions

_April 25th, 2024_

Vellum's Evaluation framework can now be used to test arbitrary functions defined in your codebase – not just
Prompts and Workflows managed by Vellum.

For example, you might test a prompt chain that lives in your codebase and that's defined using another third party
library. This can be particularly useful if you want to incrementally migrate to Vellum Prompts/Workflows, but ensure
that the outputs remain consistent.

For a detailed example of how to use Vellum's evaluation framework to test external functions, see the
[python example here](https://github.com/vellum-ai/vellum-client-python/blob/main/examples/Running%20a%20Test%20Suite%20on%20an%20External%20Function.ipynb)

## Fireworks Finetuned Models

_April 24th, 2024_

Vellum now supports models that you've fine-tuned on [Fireworks AI](https://fireworks.ai/). You can add your fine-tuned Fireworks model by navigating to the [Models page](https://app.vellum.ai/models) and clicking on the featured model template at the top.

![Fireworks Model Template](https://storage.googleapis.com/vellum-public/help-docs/fireworks_model_template.png)

Note that only the Mistral family of models are supported currently. If there are other base models that you would like to see supported, please reach out to us!

## Updated Prompt UI

_April 23rd, 2024_

We've updated the prompt editing UI throughout Vellum. You’ll see the new look in the Prompt Editor, Comparison Mode, Chat Mode, Prompt Nodes in Workflows, and Deployment Overviews. This is the first in a series of exciting improvements to the prompt editing experience that will be rolling out over the coming weeks and months.

![New Prompt Block UI](https://storage.googleapis.com/vellum-public/help-docs/new_prompt_block_ui.png)

## New Upsert Prompt Sandbox Scenario API

_April 23rd, 2024_

The API for upserting a Prompt Sandbox Scenario now requests and responds with schemas that are more consistent with
other Vellum APIs, using discriminated unions for improved type safety. This API is available on version `0.4.0` of
our SDKs.

You can find the API documentation for it [here](/api-reference/api-reference/sandboxes/upsert-sandbox-scenario).

## Function Call Input in Test Cases

_April 23rd, 2024_

Workflows support Function Call values as a valid output type. Because these function calls often come from models, it is valuable to have evaluations on these workflows that ensure that the function call output is what we expect. Test suites in Vellum now support specifying test case input and evaluation values.

![Test Case Function Call](https://storage.googleapis.com/vellum-public/help-docs/function-call-test-cases.png)

## Support for Additional Models

_April 19th, 2024_

The following models are now available in Vellum:

- Llama-3-70B-Instruct
- Llama-3-8B-Instruct
- Mixtral-8x22B-Instruct-v0.1

They can be added to your workspace through the [models page](https://app.vellum.ai/models).

## Max Tokens Warning

_April 10th, 2024_

When iterating on a Prompt in Vellum's Prompt Sandbox, you may find that its output stops mid-sentence. This is often
because the "Max Tokens" parameter is set too low, or the prompt itself is too long. To help you identify when this is
the case, we've added a warning that will appear when this max is hit.

![Max Tokens Warning](https://storage.googleapis.com/vellum-public/help-docs/max-tokens-warning.png)

## GPT-4 Turbo 04/09/2024 Model

_April 9th, 2024_

OpenAI's newest GPT-4 Turbo model `gpt-4-turbo-2024-04-09` is now available in Vellum!

## Usage Tracking in Prompt Sandbox and Prompt API

_April 9th, 2024_

We have added the ability for you to track model host usage from the `execute-prompt` [API](/api-reference/api-reference/execute-prompt#request.body.expand_meta.usage). This API update is available on version `0.3.21` of our SDKs.

You can also now view model host usage in the Prompt Sandbox by enabling the "Track Usage" toggle in your Prompt Sandbox's settings.

![Usage Tracking Sandbox](https://storage.googleapis.com/vellum-public/help-docs/usage-tracking-sandbox.png)

## New API for Listing a Test Suite's Test Cases

_April 8th, 2024_

We have a new [API](/api-reference/api-reference/test-suites/list-test-suite-test-cases) available in beta for listing the Test Cases belonging to a Test Suite at `GET /v1/test-suites/{id}/test-cases`.

This API is available on version `0.3.20` of our SDKs.

## Prompt Editor

_April 5th, 2024_

Prompt Sandboxes have an entirely new view mode: Prompt Editor. It's a dedicated space for iterating on a single Variant and Scenario. All of the features you need to work quickly are easily accessible, and collapsible sections make it simple to free up screen space. There are even more improved experiences and exciting coming down the pike for Prompt Editor, and many of those improvements will make their way into Comparison and Chat Modes, as well.

<iframe
  src='https://www.loom.com/embed/352759341cd24bc0bfb58eee7b6d0d9c?sid=e596448c-9c0f-436c-bc66-3bbce9a522e7'
  width='100%'
  height='450px'
></iframe>

## Copy and Paste Logit Bias

_April 5th, 2024_

You can now copy logit bias parameters from one Prompt Variant and paste them into another Prompt. This works in both Prompt Sandboxes and Prompt Nodes within Workflows.

![Logit Bias Copy](https://storage.googleapis.com/vellum-public/help-docs/logit-bias-copy.png)

## Test Suite Improvements

_April 4th, 2024_

We've made some changes to our Test Suite UX. Here's what's new:

- **Simplified Creation Process**: We've broken down the test suite creation into clear, manageable steps, ensuring a more guided and less overwhelming setup.
- **In-Context Editing**: You can now edit test suites directly from the Prompt or Workflow evaluations page via a new, sleek modal.
- **Enhanced Error Messaging**: We've revamped our error messages to be clearer and more actionable. You'll now receive specific feedback that pinpoints exactly where things went wrong.

![Test Suite Improvements](https://storage.googleapis.com/vellum-public/help-docs/test-suite-error.png)

## New APIs for Accessing Test Suite Runs

_April 3rd, 2024_

We have two new [APIs](/api-reference/api-reference/test-suite-runs/retrieve) available in beta for accessing your Test Suite Runs:

- A Retrieve endpoint to fetch metadata about the test suite run like it's current state at `GET /v1/test_suite_runs/{id}`
- A List executions endpoint to fetch the results of the test suite run at `GET /v1/test_suite_runs/{id}/executions`

These APIs are available on version `0.3.15` of our SDKs.
