---
title: Changelog | May, 2024
---

_May 29th, 2024_

When submitting execution Actuals for Prompts, you can now optionally include a metadata field. This field can contain arbitrary data, and will be saved and shown in the Executions tab of your Prompt Deployment.

![Prompt Actuals Metadata](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/prompt-execution-actuals-metata.png)

This is particularly helpful if you want to capture feedback/quality across multiple custom dimensions. Learn more in our
[API docs here](api-reference/api-reference/submit-completion-actuals).

## Replay Workflow from Node

_May 29th, 2024_

One of the biggest burdens when developing Workflows in Vellum is having to rerun your _entire_ Workflow whenever you want to make 
a change to just a single node and want to see its downstream effects.

You can now re-run a Workflow from a specific Node! After running a Workflow for the first time, you'll see this new play icon above each Node.

![Replay From Node Icon](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/replay-from-node-icon.png)

Doing so will re-use results from the previous execution for all upstream nodes and only actually execute the target node and all nodes downstream of it.

![Replay From Node Execution](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/replay-from-node-execution.png)

We hope this helps you decrease iteration cycles and save on LLM costs!

## Improvements to Saving Executions as Scenarios & Test Cases

_May 29th, 2024_

Saving Prompt/Workflow Deployment Executions from production API calls to an Evaluation dataset as Test Cases is a great
way to close the feedback loop between monitoring and experimentation. However, this process has historically been
time-consuming when you have many Executions to save.

We've made a number of improvements to this process:
1. You can now multi-select to bulk save Executions as Test Cases
2. We now default to the correct Sandbox/Test Suite when saving Executions as Scenarios/Test Cases
3. You'll now see warnings if the Sandbox/Test Suite you're saving to has required variables that are missing from the Execution

Check out a full demo here:
<iframe src="https://www.loom.com/embed/d7baaa2394bc49e28f2984521c95afc9"
  width="100%"
  height="450px"
></iframe>

## Prompt Sandbox History Update

_May 28th, 2024_

Previously, editing past versions of a Prompt Sandbox could be confusing, with unclear indications of which version you were modifying and how it was being saved.

Now, the history view for a Prompt Sandbox is read-only. To edit a previous version, simply click the Restore button, and a new editable version will be created from that specific version.

## Workflow Deployment Actuals Metadata

_May 28th, 2024_

When submitting execution Actuals for Workflows, you can now optionally include a metadata field. This field can contain arbitrary data, and will be saved and shown in the Executions tab of your Workflow Deployment.

This is particularly helpful if you want to capture feedback/quality across multiple custom dimensions.

```
curl -X POST https://predict.vellum.ai/v1/submit-workflow-execution-actuals \
     -H "X_API_KEY: "$VELLUM_API_KEY"" \
     -H "Content-Type: application/json" \
     -d '{
  "execution_id": "be975a69-33c7-4ff0-b6ac-d8008198db1e",
  "actuals": [
    {
      "output_type": "STRING",
      "output_key": "final-output",
      "quality": 0.8,
      "metadata": {
        "user_score": 1.0,
        "internal_score": 1.0,
        "internal_notes": "The output was not factually correct."
      }
    }
  ]
}'
```

## Guardrail Workflow Nodes

_May 23rd, 2024_

You can now use Evaluation Metrics inside of Workflows with the new Guardrail Node! Guardrail Nodes let you run pre-defined evaluation criteria at runtime as part of a Workflow execution so that you can drive downstream behavior based on that metric's score.

For example, if building a RAG application, you might determine whether the generated response passes some threshold for [Ragas Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html) and if not, loop around to try again.

![Guardrail Nodes](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/guardrail-nodes.png)

## Chat Mode Revamp

_May 22th, 2024_

Chat Mode in Prompt Sandboxes has received a major facelift! The left side of the new interface will be familiar to anyone using the Prompt Editor, while the rest of the interface retains its functionality with a fresh new look. We've also fixed some UX wonk and minor bugs during the restyling process.

![Chat Mode Styling Update](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/chat-mode-styling-update.png)

## Double-Click to Resize Rows & Columns in Prompt Sandboxes

_May 22th, 2024_

You can now double-click on resizable row and column edges in both Comparison and Chat modes to auto-expand that row/column to its maximum size. If already at maximum size, double-clicking will reset them to their default size. Additionally, in Comparison mode, double-clicking on cell corners will auto-resize both dimensions simultaneously.

## Improved Image Support in Chat History Fields

_May 22th, 2024_

We've made several changes to enhance the UX of working with images. Chat History messages now include an explicit content-type selector, making it easier to work with image content using supported models. You can now add publicly-hosted images in multiple ways: by pasting an image URL, pasting a copied image, or dragging and dropping an image from another window.

Additionally, we've added limited support for embedded images. You can embed an image directly into the prompt by copy/pasting or dragging/dropping an image file from your computer's file browser. This method has a 1MB size limit and is an interim solution as we continue to explore image upload and hosting options.

<div style="display: grid; grid-template-areas: 'a b' 'c c'; gap: 12px">
  <div style="grid-area: a">
    <img src="https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/improved-image-support.png" />
  </div>
  <div style="grid-area: b">
    <img src="https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/content-type-select.png" />
  </div>
  <div style="grid-area: c">
    <img src="https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/image-config-modal.png" />
  </div>
</div>

## Gemini 1.5 Flash

_May 20th, 2024_

Google's [Gemini 1.5 Flash model](https://deepmind.google/technologies/gemini/flash/) is now available in Vellum. You can add it to your workspace from the [models page](https://app.vellum.ai/models).

## Llama 3 Models on Bedrock

_May 14th, 2024_

We now support both of the Llama 3 models on AWS Bedrock. You can add them to your workspace from the [models page](https://app.vellum.ai/models).

## GPT-4o Models

_May 13th, 2024_

OpenAI's newest [GPT-4o models](https://openai.com/index/hello-gpt-4o/) `gpt-4o` & `gpt-4o-2024-05-13` are now available in Vellum and have been added to all workspaces!

![GPT 4o](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/gpt-4o.png)

## Organization and Workspace Names in Side Nav

_May 13th, 2024_

You can now view the active Organization's name and the active Workspace's name in the left sidebar navigation.

![Workspace and Org Name Nav](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/org-name-sidebar.png)

## Run All Button on Evaluation Reports

_May 10th, 2024_

There's now a "Run All" button on evaluation reports that runs a test suite for all variants. Instead of running each variant individually, you can now run them all with one click.

![Prompt Node Execution](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/run-all-button.png)

## Prompt Node Monitoring

_May 9th, 2024_

Vellum is now capturing monitoring data for deployed Prompt Nodes. Whenever a deployed Workflow invokes a Prompt Node, it will now show a link displaying the Prompt Deployment label:

![Prompt Node Monitoring](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/prompt-node-monitoring.png)

Clicking on the link will take you to the _Prompt's executions_ page, where you can then see all metadata captured for the execution, including the raw request data sent to the model:

![Prompt Node Execution](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/prompt-node-execution.png)

## Groq Support

_May 9th, 2024_

Vellum now has a native integration with the LPU Inference Engine, [Groq](https://groq.com/). All public models on Groq are now available to add to your workspace. Be sure to add your API key as a Secret named `GROQ_API_KEY` on the [API Keys page](https://app.vellum.ai/api-keys).

Groq is an LLM hosting provider that offers incredible inference speed for open source LLMs, including the recently released (and very hyped!) [Llama 3](https://llama.meta.com/llama3/) model.

![Groq Support](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/groq-support.png)

## Function Calling in Prompt Evaluation

_May 8th, 2024_

Prompts that output function calls can now be evaluated via Test Suites. This allows you to define Test Cases consisting of the inputs to the prompt, and the expected function call, then assert that there's a match. For more, check out our [docs](/help-center/evaluation/quantitative-evaluation#function-calling).

![Function Call Prompts](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/function-tests-edit.png)

## Out-of-Box Ragas Metrics

_May 7th, 2024_

Test-driven development for your RAG-based LLM pipelines is now easier than ever within Vellum!

Three new [Ragas metrics](https://docs.ragas.io/en/latest/index.html) – [Context Revelancy](https://docs.ragas.io/en/latest/concepts/metrics/context_relevancy.html), [Answer Relevance](https://docs.ragas.io/en/latest/concepts/metrics/answer_relevance.html) and [Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html) – are now available out-of-box in Vellum. These can be used within Workflow Evaluations to measure the quality of a RAG system.

For more info, check out our new help center article on [Evaluating RAG Pipelines](/help-center/evaluation/evaluating-rag-pipelines).

![Ragas metrics](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/ragas-metrics.png)

## Subworkflow Node Streaming

_May 7th, 2024_

Subworkflow Nodes can now stream their output(s) to parent workflows.

This allows you to compose workflows using modular subworkflows without sacrificing the ability to delivery incremental results to your end user.

Note that only nodes immediately prior to Final Output Nodes can have their output(s) streamed.

![Subworkflow Streaming](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/subworkflow-streaming.gif)

## Default Test Case Concurrency in Evaluations

_May 4th, 2024_

You can now configure how many Test Cases should be run in parallel during an Evaluation. You might lower this value
if you're running into rate limits from the LLM provider, or might increase this value if your rate limits are high.

![Test Case Concurrency](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/default-test-case-concurrency.png)
