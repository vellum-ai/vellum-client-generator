---
title: Changelog | May, 2024
---

## Groq Support

Vellum now has a native integration with the LPU Inference Engine, [Groq](https://groq.com/). All public models on Groq are now available to add to your workspace. Be sure to add your API key as a Secret named `GROQ_API_KEY` on the [API Keys page](https://app.vellum.ai/api-keys).

Groq is an LLM hosting provider that offers incredible inference speed for open source LLMs, including the recently released (and very hyped!) [Llama 3](https://llama.meta.com/llama3/) model.

![Groq Support](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/groq-support.png)

## Function Calling in Prompt Evaluation

Prompts that output function calls can now be evaluated via Test Suites. This allows you to define Test Cases consisting of the inputs to the prompt, and the expected function call, then assert that there's a match. For more, check out our [docs](/help-center/evaluation/quantitative-evaluation#function-calling).

![Function Call Prompts](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/function-tests-edit.png)

## Out-of-Box Ragas Metrics

_May 7th, 2024_

Metric driven development for your LLMs and RAG pipelines is now easier than ever within Vellum!

Three new [Ragas metrics](https://docs.ragas.io/en/latest/index.html) - [Context Revelancy](https://docs.ragas.io/en/latest/concepts/metrics/context_relevancy.html), [Answer Relevance](https://docs.ragas.io/en/latest/concepts/metrics/answer_relevance.html) and [Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html) are now available Out-of-Box in Vellum to ensure quality of your LLM systems.

For more info, check out our new help center article on [Evaluating RAG Pipelines](/help-center/evaluation/evaluating-rag-pipelines).

![Ragas metrics](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/ragas-metrics.png)

## Subworkflow Node Streaming

_May 7th, 2024_

Subworkflow Nodes can now stream their output(s) to parent workflows.

This allows you to compose workflows using modular subworkflows without sacrificing the ability to delivery incremental results to your end user.

Note that only nodes immediately prior to Final Output Nodes can have their output(s) streamed.

![Subworkflow Streaming](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/subworkflow-streaming.gif)

## Default Test Case Concurrency in Evaluations

_May 4th, 2024_

You can now configure how many Test Cases should be run in parallel during an Evaluation. You might lower this value
if you're running into rate limits from the LLM provider, or might increase this value if your rate limits are high.

![Test Case Concurrency](https://storage.googleapis.com/vellum-public/help-docs/changelogs/2024-05/default-test-case-concurrency.png)
