---
title: Building Common LLM architectures with Vellum Workflows
description: Discover how to build dynamic architectures using Workflows, from RAG systems to message routing and looping
---

With a large number of supported node types (full details here: [Experimenting with Workflows](/help-center/workflows/experimentation)) and few limits on how they can be connected to each other, the types of architectures/ applications you can create using Workflows is very large.

The list of architectures below is not exhaustive, we’re continuing to build it out. If you come up with an interesting architecture that you think the community might benefit from, please reach out so we can add it to the list here.

## Create a Retrieval Augmented Generation (RAG) system

LLM applications often require specific context from a Vector DB which is added into the prompt. Forget signing up for multiple systems and being stuck on various micro decisions, with Vellum you can prototype a RAG system in minutes

<iframe
  src="https://app.vellum.ai/public/workflow-deployments/090744e8-3ec0-4047-ba98-c1445068397f"
  width="80%"
  height="350px"
  style="margin: 24px auto;"
  experimental_enableRequestFullscreen={true}
/>

<Steps>

### Create a Document Index and upload your documents
Follow this article for tips: [Uploading Documents](/help-center/documents/uploading-documents))

### Add a Search Node in your Workflow

Place this anywhere and connect it to the "entrypoint"

### Add a Prompt Node 

The prompt node should take the results of your Search Node as an input variable

### Link to a Final Output or other downstream node
For example, if the Prompt Node result is a certain value branch execution based on a Conditional Node)

### Set up input variables and hit Run!

</Steps>

## Route messages to a Human

If you’re building an agent that answers questions coming from users (e.g., a support chatbot), you may want to set up rules such that anytime the incoming message from a user is sensitive (e.g., the user is angry or in a dangerous situation) then the LLM automatically escalates it to a human. With Workflows you’d be able to build that out real quick.

<iframe
  src="https://app.vellum.ai/public/workflow-deployments/d213be0a-65f0-407b-bafc-76077bfa3692"
  width="80%"
  height="350px"
  style="margin: 24px auto;"
  experimental_enableRequestFullscreen={true}
/>

<Steps>

### Create a classification prompt
Use a Prompt Node to filter out incoming messages

### Create a downstream prompt 
Use another prompt node for the LLM to respond to messages that don’t need to be escalated

### Create and connect two Final Output Nodes
Connect the classification prompt outputs to two separate Final Output Nodes

### Set up variables and hit Run!

</Steps>

## Retrying a Prompt Node in case of non-deterministic failure

Prompt nodes support two selectable outputs - one from the model in case of a valid output and one in case of a non deterministic error. Model hosts fail for all sorts of reasons that include timeouts, rate limits, or server overload. You could make your production-grade LLM features resilient to these features by adding retry logic into your Workflows!

<iframe
  src="https://app.vellum.ai/public/workflow-deployments/8c3fac0e-b991-43f5-846a-40a754234feb"
  width="80%"
  height="350px"
  style="margin: 24px auto;"
  experimental_enableRequestFullscreen={true}
/>

<Steps>

### Add a standard Prompt Node

### Add a Conditional Node (`Error Check`)
This node will read from the new Error output from the Prompt Node and check to see if it's _not null_.

### Define another Conditional Node (`Count Check`)
This node will read from the Prompt Node's Execution Counter, and check if it's been invoked more than your desired limit (`3`).

### Loop back to the Prompt Node
Loop back to the Prompt Node if it's under the limit, or exit with some error message if it's over the limit. In the case that the error is null, exit with the Prompt Node's response.

</Steps>

## Summarizing the contents of a PDF file

Vellum Document Indexes are typically used to power RAG systems via Search Nodes. However, they can also be used to operate on the entirety of a single file's contents.
In this example, we make use of Vellum Document Indexes not for the purpose of search, instead, to leverage the OCR that's performed and operate on the raw text that's extracted
from a PDF file.

Prerequisites:
You need to have ....
- Created a Document Index. Note: it doesn't matter what embedding model or chunking strategy you choose, since we're only leveraging the OCR capabilities of the Document Index.
- Uploaded a PDF file to the Document Index and noted down its ID.
- Generated a Vellum API Token and saved its value as a Workspace Secret.

<iframe
  src="https://app.vellum.ai/public/workflow-deployments/94f60249-8729-4c10-bb54-941621abceaa"
  width="80%"
  height="350px"
  style="margin: 24px auto;"
  experimental_enableRequestFullscreen={true}
/>

<Steps>

### Set the input to the workflow

This will be the ID of a Document that was previously uploaded to a Document Index

### Add a Templating Node (`Document API URL`)

This will construct the url of a Vellum API we want to hit.

### Add an API Node (`Document API`) 

This will ping the Vellum API and retrieve metadata about the Document.

### Add a Templating Node (`Processed Document URL`)

This will extract the url of the processed document from the API response.

### Add an API Node (`Processed Document Contents`)

This will retrieve the text contents of the Document.

### Pass those contents to a Prompt Node that summarizes the text.

</Steps>
