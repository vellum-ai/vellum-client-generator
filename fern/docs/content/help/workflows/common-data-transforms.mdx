The Templating Node supports [Jinja2](https://jinja.palletsprojects.com/en/3.1.x/templates/) syntax and is a flexible way of performing light-weight data transformations as part of your Workflow. Here are some common data manipulations you may want to make in a Workflow and how you define them via Templating Nodes.

# String Manipulation

### Output Only the First n Characters

Useful if you want to ensure that you’re not providing too much context to a prompt.

![String Manipulation](https://vellum-ai.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F31726ad7-4cdb-4a88-bf83-6540efb92c1a%2F2023-08-25_13-57-24.png?table=block&id=454c438f-e20a-424f-96eb-007cb7b64662&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=1770&userId=&cache=v2)


# JSON Manipulation

### Checking LLM Output for Valid

If you’re trying to extract structured JSON from unstructed text using a prompt, or if you want to use OpenAI’s function-calling functionality, it’s likely you’ll need to check whether an LLM’s response is valid JSON and if so, convert the output string as proper JSON.

Here’s how to do it:

![JSON Manipulation](https://vellum-ai.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa5758616-810b-43e0-bd69-3b6d7721940a%2F2023-08-25_18-10-44.png?table=block&id=a4e2edc4-3c36-4407-b1fa-60681c1f9983&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=1770&userId=&cache=v2)


# Chat History Manipulation

### Output the Most Recent n Messages in Chat History

If you’re building a chatbot and conversations can be long-lived, you may find that your chat histories are too long to fit within the context window of a prompt.

Once simple solution is to only ever include the most recent `n` messages from the conversation. Here’s how you can do this:

![Chat History Manipulation](https://vellum-ai.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F8abac24f-f598-4016-abdc-f87baf354da8%2F2023-08-25_18-27-15.png?table=block&id=d8e81e3d-9ae1-4ceb-b6df-87fcf193aa2e&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=1770&userId=&cache=v2)


# Search Result Manipulation

### Citing Sources via Chunk Concatenation Customization

Search Nodes make it easy to query a vector store for text that’s semantically similar to some input. By default, the chunks of text that are returned are concatenated together into a single string using a configurable separator (e.g. `\n\n#####\n\n`). The flattened string can then be fed directly to Prompt Nodes as an input variable and referenced within your prompt template.

However, if you want your Prompt to cite its sources and say where it got the info it used to generate its response, then you’ll need more than just the chunk text. You need the name/id/url/etc of the document each chunk came from and you need to provide this info to your Prompt in a consumable form. This is where Templating Nodes come in.

The template below takes in the raw search results and performs custom chunk concatenation, but also pulls in info from the document associated with each chunk.

![Search Result Manipulation](https://vellum-ai.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F8c68490d-c1c3-4c9e-bb52-8b64d1294257%2F2023-08-30_20-15-38.png?table=block&id=6ce6571f-8479-4748-b1ed-58d28a3e34ad&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=1770&userId=&cache=v2)


# Need Help?

Templating nodes are flexible and powerful, but admittedly not the most intuitive. If you’d like to see additional examples here, or have ideas for custom filters that we should add (like the `is_valid_json_string` filter [used above](/help-center/workflows/common-data-transforms#checking-llm-output-for-valid)), please don’t hesitate to [reach out to us on discord](https://discord.gg/6NqSBUxF78)!