---
title: Create Custom Metrics for LLM Evaluation in Vellum
description: Learn how to create custom Metrics to evaluate your LLM Workflows with ease. Catch edge-cases, prevent regressions, and ship AI features faster with more confidence!
---
## Create Custom Reusable Evaluation Metric

In addition to the default metrics, Vellum makes it easy to define custom Reusable Evaluation Metrics tailored to your specific business logic and use-case.
This saves you time and ensures standardized evaluation criteria for your Prompts, Workflows, or external entities you'd like to test.

Let's create your first Reusable Evaluation Metric

1. Visit the [Evaluations tab](https://app.vellum.ai/evaluations) in Vellum and open the Metrics page
2. Click the blue Create Metric button at the top-right of the page to open the Create Metric modal
3. From the metric type dropdown, select JSON Schema Match. To learn about metric types other than JSON Schema Match, see Vellum's [Available Metric Types](#available-metric-types).
4. In the "Label" field at the top left, enter "My First Metric". The "Name" field should autopopulate. This is a unique name that you can use to programmatically identify this metric.
5. In the "Description" field, type in "My first metric description"
6. Click next to configure your metric and define what the expected output should match
7. Add "name" and "email" properties to the JSON schema
8. Click Finish to exit the modal and see your newly added metric card on the Metrics page

Congrats! You've now created a Reusable Evaluation Metric that will be visible when selecting and configuring Evaluation Metrics within any Test suite.

![Create New Reusable Evaluation Metric](https://storage.googleapis.com/vellum-public/help-docs/quantitatively-evaluating-outputs/metric-creation.png)

## Available Metric Types

### JSON Schema Match

Check that the output matches a specified JSON schema.

Returns a score of 1 if the output matches the schema, and 0 otherwise.

### Workflow

Run a Workflow to evaluate the output.

See [Workflow Evaluation Metric](/help-center/evaluation/workflow-evaluation-metric) for more details.

### Code

Run custom Python code to evaluate the output.

The code must include a function named `main` that takes the function arguments specified when creating the metric and returns a dictionary with keys `score` and optionally `normalized_score`.

<CodeBlock title="Example">
```python
def main(input_1, input_2, target, completion):
    return {
        "score": 10,
        "normalized_score": 0.5,  // Optional, must be in the interval [0, 1] if provided
    }
```
</CodeBlock>
