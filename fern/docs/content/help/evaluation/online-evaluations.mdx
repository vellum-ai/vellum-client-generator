---
title: Leveraging Online Evaluations for LLM Development with Vellum
description: Learn how Vellum's Online Evaluations feature continuously assesses LLM outputs, ensuring model quality across diverse deployment scenarios.
---

Online Evaluations in Vellum provide a powerful way to continuously assess the quality of your deployed LLM applications. 
This feature allows you to monitor and evaluate the performance of your prompts or workflows in real-time as they're being used in production.

# Getting Started with Online Evaluations

## Step 1: Create and Deploy Your LLM Application

1. Start by creating either a workflow in the Workflow Sandbox or a prompt in the Prompt Sandbox.
2. Once you're satisfied with your creation, deploy your workflow or prompt.

## Step 2: Configure Metrics

After deployment, you can configure metrics to evaluate your LLM application's performance:

1. Navigate to your deployed prompt or workflow.
2. Locate the "Metrics" tab, which is conveniently positioned between the "Releases" and "Monitoring" tabs.
3. In the Metrics tab, configure your metrics according to your specific needs.

Once your metrics are configured, you can view how your executions are evaluated:

1. Go to the "Executions" tab of your deployment.
2. Click on an individual execution ID to view its details.
3. In the execution details, you'll find the evaluation results based on your configured metrics.

<Note>For detailed information on configuring metrics, refer to the Evaluations and Metrics sections in our documentation.</Note>

## Step 3: Save and Activate

Once you've configured your metrics, save your changes. 
From this point forward, every execution of your deployment will be automatically evaluated against these metrics, providing you with ongoing, real-time performance insights.

# Understanding Online Evaluations

Online Evaluations offer several key benefits for LLM application development:

1. **Real-time Performance Monitoring**: Continuously assess your deployment's performance as it handles live requests.
2. **Quality Assurance**: Ensure your LLM application maintains high standards even as input patterns may shift over time.
3. **Regression Detection**: Quickly identify any degradation in performance, allowing for swift corrective action.
4. **Insight-Driven Improvement**: Use the gathered data to inform future iterations and improvements of your LLM application.

# Viewing Evaluation Results

To access your Online Evaluation results:

1. Go to your deployment's details page.
2. Navigate to the "Executions" tab.
3. Click on an individual execution ID to view its details.
4. In the execution details, you'll find the evaluation results based on your configured metrics.


You can analyze these results to gain insights into your LLM's strengths and areas for improvement.

# Advanced Usage

## Multiple Metrics

You can configure multiple metrics for a single deployment to evaluate different aspects of your deployment's performance. This allows for a more comprehensive assessment of your deployment's capabilities.

## API Access

For those who prefer programmatic access, Vellum provides API endpoints to retrieve Online Evaluation results. This enables easy integration with your existing monitoring and analytics systems.

# Conclusion

Online Evaluations in Vellum offer a robust, automated way to ensure the ongoing quality and performance of your LLM applications. By providing continuous, metric-based assessments, this feature empowers you to maintain high standards and make data-driven improvements to your prompts and workflows.

Remember, the key to leveraging Online Evaluations effectively is in thoughtfully configuring your metrics to align with your specific use case and quality standards. Regularly reviewing and adjusting these metrics will help you get the most out of this powerful feature.