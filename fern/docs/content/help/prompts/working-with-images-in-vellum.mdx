---
title: Leverage Images in Your Vellum Prompts and Workflows
description: Learn how to send images to multimodal models like GPT-4 Turbo with Vision from within Vellum’s UI
---

Leverage the power of multimodal models to process both natural language and visual inputs within your LLM-applications.

Vellum offers image support for OpenAI’s vision models like GPT-4 Turbo with Vision, `gpt-4-vision-preview`, both via [API](https://docs.vellum.ai/api-reference/) and in the Vellum UI.
In the UI, Vellum supports publicly hosted images that meet the image specifications set forth by OpenAI’s vision models (and outlined in this article). Vellum supports images as inputs to both your Prompts and Workflows.

Here’s how to get started!

![Images in Vellum UI](https://storage.googleapis.com/vellum-public/help-docs/images-in-prompts-walkthrough.gif)

<Callout intent="info">
  Are you looking to use other multimodal models beyond `gpt-4-vision-preview`
  within Vellum? Let our team know so we can prioritize it!
</Callout>

## Using Images in the UI

In the Prompt and Workflow Sandbox, images can be added in Chat History messages as part of the scenario input.

Begin by selecting the correct model, `GPT-4 Turbo with vision`, in your Prompt or in a Workflow sandbox Prompt Node.

![Vision Model Selection](https://storage.googleapis.com/vellum-public/help-docs/images-model-selection.png)

Next, to add an image, drag and drop any valid image to one of scenario Chat History messages. This convert the Chat Message into an array that can contain multiple image and/or text items.

Here’s what this looks like each sandbox:

Prompt Sandbox

![Prompt Sandbox Steps](https://storage.googleapis.com/vellum-public/help-docs/images-prompt-steps.png)

![Images in Prompt Scenarios](https://storage.googleapis.com/vellum-public/help-docs/images-in-prompts.png)

Workflow Sandbox

![Workflow Sandbox Steps](https://storage.googleapis.com/vellum-public/help-docs/images-workflow-steps.png)

![Images in Workflow Scenarios](https://storage.googleapis.com/vellum-public/help-docs/images-in-workflows.png)

Once you’ve added in your image, you can configure its settings by clicking the small gear icon to the right of the image. Here you'll be able to adjust things like the `Image Detail` which can have a big impact on token usage (more on that below).

![Image Configuration Steps](https://storage.googleapis.com/vellum-public/help-docs/images-config.png)

You can also switch out an image you’ve dragged in for a new one by updating the image URL in the settings.

![Image Configuration Modal](https://storage.googleapis.com/vellum-public/help-docs/images-config-modal.png)

**Image Specifications**

Here are some important model specifications from GPT-4 Turbo with Vision model to keep in mind as you’re incorporating images into your Prompts and Workflows:

- **Number of Images:** No set limit

  There is not a fixed number here but token and image size restrictions still apply to determine the number of images that can be sent.

- **Image Size:** Less than 20MB

  GPT 4 Turbo with Vision currently has an image size limit of 20MB. For prompts and workflows with multiple images, the combined size should be smaller than 20MB

- **Supported Image Filetypes:**

  - JPEG (.jpeg / .jpg)
  - PNG (.png)
  - Non-animated GIF (.gif)
  - WEBP (.webp)

- **Other Notes**

  - GPT-4 Turbo with vision does not currently support tool calls so adding function blocks in the `$chat_history` will cause the model to fail
  - The Vellum UI currently only supports publicly hosted image urls. To send an image file directly, you can use the Vellum API instead. Here's a quick example on how to send an image to the model, using Vellum's Python SDK:

    ```python
    image_link = "https://storage.googleapis.com/vellum-public/help-docs/add_prompt_block_button.png"
    response = client.execute_prompt(
        prompt_deployment_name="github-loom-demo",
        inputs=[
            PromptDeploymentInputRequest_ChatHistory(
                name="$chat_history",
                value=[
                    ChatMessageRequest(
                        role=ChatMessageRole.USER,
                        content={
                            "type": "ARRAY",
                            "value": [
                                {"type": "STRING", "value": "What's in this image?"},
                                {"type": "IMAGE", "value": {"src": image_link}},
                            ],
                        },
                    )
                ],
                type=VellumVariableType.CHAT_HISTORY,
            ),
        ],
    )
    print(response.outputs[0].value)
    ```

<Callout intent="info">
  Valid image URLs: The image being dragged into the UI must be the absolute
  path that includes the image filetype in the URL (example:
  https://storage.googleapis.com/vellum-public/help-docs/release-tags-on-deploy.png)
</Callout>

**Image Detail and Token Usage**

When working with image models, token usage is an important factor to consider. For GPT-4 Turbo with Vision, the two main factors are the image’s size and it’s detail setting.

There are three possible settings for the image detail: `low`, `high`, or `auto`

![Image Details](https://storage.googleapis.com/vellum-public/help-docs/images-config-detail.png)

In Vellum, the default detail setting is `low` to prevent unintended token usage. OpenAI has this default setting at `auto` where the model decides whether to use low or high detail based on the size of the input image.

The `low` setting processes a lower resolution `512x512` version of the image. With low, the response time is faster and there’s a fixed token consumption per image. At the time of this writing, that amount is `85 tokens`. The low setting is great when the fine details of the image are not required.

The `high` setting on the other hand is the high resolution mode. In this mode, the input image is tiled and a detailed segment is created from it. Token usage is calculated based on the number of these segments which correlates to the image size. High resolution allows for a more comprehensive interpretation of your image.

You can learn more about the image detail setting and OpenAI Vision models on [their site](https://platform.openai.com/docs/guides/vision)
