Evaluating model quality is challenging because the responses generated by LLMs is inherently
probabilistic when temperature is non-zero. Testing output quality before it goes in production
(unit testing) and testing while making changes in production (regression testing) are both
very important to ensure a high quality end user experience. We’ve covered our perspective on testing
LLM quality in our blog [here](https://www.vellum.ai/blog/how-to-evaluate-the-quality-of-large-language-models-for-production-use-cases).

There are 3 kinds of testing you can do in Vellum:

1. Unit testing with 2-10 scenarios (in Playground)
2. Unit testing with more than 10 scenarios (Test Suites)
3. Back-testing while changing prompts in production (Deployments)

Let's take a look.

# Quantitative Evaluation During Rapid Iteration

The Playground is meant for rapid iteration between multiple models/prompts.
We’ve usually seen people spend hours in here iterating on prompts and model
providers until they find a prompt that clears their test cases.

While the Prompt Playground is primarily meant for visually inspecting a handful
of scenarios, you can also use it to quantitatively evaluate model quality.

## Initial Set Up

Toggle the Evaluate Outputs button on to start evaluating output quality

![Initial Set Up](https://vellum-ai.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffa8ed485-aa83-4dcf-958f-9b26a97f8c2c%2FScreen_Shot_2023-05-03_at_3.36.38_PM.png?table=block&id=4c309d1a-2a0f-46fe-a692-7d36fbdb8ae0&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=670&userId=&cache=v2)


Vellum currently supports 4 evaluation operators:

- Exact match (best used for classification use cases)
- Regex match (best used for data extraction use cases)
- Semantic similarity (best used for creative/generative use cases)
- Webhook (best used for custom use cases and bespoke evaluation)


## Defining Target Responses

You’re expected to include the target/correct response for each scenario while evaluating output quality.

![Defining Target Responses](https://vellum-ai.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0e7c1b7b-9060-4b20-a594-175b3aa51c5d%2FScreen_Shot_2023-05-03_at_3.49.16_PM.png?table=block&id=c44c3868-f60f-499e-81eb-6e8dcb57ebcd&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=580&userId=&cache=v2)

## Interpreting Output Quality

Vellum will compare the model generated response for each prompt against the target response. For semantic similarity, we measure how close the model generated response is in meaning to the target response. Regex match & Exact match have either a 0 or 1 response for each cell.

Each cell gets its own score and the total for each model/prompt combination (column) is averaged at the top. You can use this feature to see how close, on average, each model/prompt combination is to the target response you’d expect

![Interpreting Output Quality](https://vellum-ai.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb727e0a6-dabe-4144-8768-665a89d7d7f3%2FScreen_Shot_2023-05-03_at_3.58.38_PM.png?table=block&id=f34588e5-28cf-4d46-b1f0-30d781f9b2e8&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=860&userId=&cache=v2)


# Bulk Quantitative Evaluation via Test Suites
Manual evaluation and spot checking begins to break down when you have more than ~10 scenarios.
This is where Test Suites come in. Test Suites are a way to define a large number of scenarios and evaluate them in bulk.
You typically run them once you think your Prompt is getting to a reasonably stable state and you're ready to evaluate
it against a large number of scenarios.

Let's take a look at how to set them up and use them in Vellum.

## Initial Set Up

### Creating a Test Suite
Go to the <a href="https://app.vellum.ai/test-suites" target="_blank">Test Suites tab</a> and click on Create Test Suite.

![Test Suites](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb312c988-e594-47a8-8aaa-78ee276f6782%2FScreen_Shot_2023-05-03_at_4.45.14_PM.png?table=block&id=c4862144-b7a0-4490-ad25-2b3c503eed9e&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=1060&userId=242952a1-fd51-4ea4-92e7-f832187512ce&cache=v2)

Fill out details about the test suite on the next screen.
- Label / Name: How would you like to refer to this Test Suite?
- Expected Prompt Variables: The variables that are passed into the prompt at run-time. **Note:** the variable names here need to be exactly the same as the variable names in Playground.
- Evaluation Metric: Choose how you would like to compare the model generated response with the target response.
    - Exact Match: Best for classification tasks where there's a definitive correct answer.
    - Regex Match: Best for tasks where you know the exact structure/syntax of the output
    - Semantic Similarity: Best used when the exact wording of the output doesn't matter, as long as it conveys the intended meaning.
    - Webhook: Best used for custom use cases and bespoke evaluation

### Defining Test Cases
The way Test Cases work is you define a set of example "Input Values" and a "Target" value.
Then, when the Test Suite is executed, Vellum iterates over the Test Cases and for each, uses the Input Values
and a Prompt to generate some output. It then compares that output to your defined target value using the Test Suite's
configured evaluation metric.

For example, if your Test Suite is configured to use Exact Match, then Vellum says "If the model generated output is exactly
the same as the target output, then this Test Case passes. Otherwise, it fails."

Or, if your Test Suite is configured to use Semantic Similarity, then Vellum says "The more semantically similar the
model's generated output is to the target value, then the higher the score for this Test Case."

You can define Test Cases in one of three ways:
1. Manually via the UI
2. Via a CSV upload
3. Via an API call

![Defining Test Cases](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F017b3f91-d5b8-47e3-a823-1c825888be96%2FScreen_Shot_2023-05-03_at_5.03.16_PM.png?table=block&id=088b0739-8c71-4231-aa64-fee82cfcaaf1&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=1060&userId=242952a1-fd51-4ea4-92e7-f832187512ce&cache=v2)

### Running a Test Suite
Once you've defined your Test Suite and Test Cases, you can use it in a Prompt. Go to that Prompt, scroll to the bottom,
and click on the Test Suite dropdown. Select the Test Suite you want to include.

![Using a Test Suite in a Prompt](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F353c0731-fc01-426e-b934-932cc3adc6b2%2FScreen_Shot_2023-05-03_at_5.06.04_PM.png?table=block&id=8fb20746-6885-405b-8e8e-30ea2088a2c7&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=1150&userId=242952a1-fd51-4ea4-92e7-f832187512ce&cache=v2)

You can then run the Test Suite against one or more Prompt Variants. Running each Variant against the Test Suite
shows how close each model generated response is to the target response and an average for the whole prompt.
This should give you a quick visual indicator on which prompt does well across all your test cases! You can toggle
between seeing the numerical scores vs the actual output, and can export csvs of the results.

![Running a Test Suite](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F16eb7967-ab26-4a9f-8984-3b0fe8b61b68%2FScreen_Shot_2023-05-03_at_5.10.00_PM.png?table=block&id=180dde88-3794-427f-b79e-83f6ec23b838&spaceId=71c05e3e-272b-4acf-9889-90a304d95d06&width=2000&userId=242952a1-fd51-4ea4-92e7-f832187512ce&cache=v2)

